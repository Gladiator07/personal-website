[
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "About",
    "section": "",
    "text": "This page contains a detailed overview of the past work I did."
  },
  {
    "objectID": "pages/about.html#work-experience",
    "href": "pages/about.html#work-experience",
    "title": "About",
    "section": "üíº Work Experience",
    "text": "üíº Work Experience\n\nWolters Kluwer (Jan 2023 - Present) - Machine Learning Engineer\n\nBorrower Analytics 2.0 (UCC filings)\n\nProcessed 65M UCC filings for information extraction from PDFs (&gt;1B single-page images).\nFine-tuned and deployed a vision-language model to extract 50+ fields from UCC1/UCC3 across diverse state formats; achieved 95% extraction accuracy.\nDeployed with a high-throughput serving stack at ~10 images/sec on H100 GPUs.\nBuilt an LLM-as-a-judge framework to generate high-quality labeled data, significantly reducing human labeling effort.\nTrained and deployed a transformer for text segmentation (97% recall) extracting complete collateral text.\nTrained a lien-classification model (95% accuracy) based on collateral text.\n\nIRA Knowledge-Base Chatbot (RAG)\n\nBuilt a RAG chatbot over an XML-based IRA knowledge base for faster query resolution.\nImplemented query transformation, parent‚Äìchild retrieval, hierarchical indexing, and multi-vector retrieval in LanceDB.\nImproved retrieval with ColBERT reranking and hybrid search (semantic + BM25).\nReached ~97% CSAT, improving user experience and response accuracy.\n\nWK AI Studio (internal fine-tuning platform)\n\nBuilt an end-to-end internal framework to fine-tune and deploy models to production.\nEnabled non-technical teams to fine-tune models with their own data.\nIntegrated MLflow with full lineage, model registry, and dataset/model versioning for production tracking.\nSupported distributed multi-GPU, 4-/8-bit training, LoRA/QLoRA, mixed precision, and differential learning rates, plus other SOTA techniques.\n\nBLMS (Business License Match/Search)\n\nBuilt a search engine to recommend required licenses for starting a business.\nDeveloped a HyDE (Hypothetical Document Embeddings) workflow to expand short user queries into meaningful search text.\nUsed synthetically generated descriptions from an internal taxonomy and a reranker, achieving 96% retrieval accuracy.\n\nProviso (legal citations chatbot)\n\nBuilt a RAG-based chatbot on 91k legal citations for efficient legal query resolution.\nEnhanced retrieval with metadata filtering, query transformation, and similar techniques as IRA.\nUsed BERTTopic to generate hypothetical topics, which were leveraged for metadata filtering to reduce search space and improve query handling.\n\nEarlier ‚Äî Data Science Intern (Jan‚ÄìJul 2023)\n\nBuilt an in-house key information extraction solution using a transformer-based stack.\nShipped a document classification pipeline.\n\n\n\n\nWeights & Biases (May 2022 - Present) - Ambassador\n\nEngineered optimized Kaggle notebooks with integrated W&B tracking and monitoring.\nAuthored technical reports showcasing W&B across medical imaging, visual-language models (Flamingo, BLIP-2), few-shot learning (SetFit), PyTorch 2.0, Hugging Face, RAG, and LLMs.\nYou can find all my W&B blogs here"
  },
  {
    "objectID": "pages/about.html#competitions",
    "href": "pages/about.html#competitions",
    "title": "About",
    "section": "üìà Competitions",
    "text": "üìà Competitions\nI love participating in machine learning competitions. I am primarily active on Kaggle and am a Kaggle Competition Expert.\n\nWSDM Cup ‚Äî Multilingual Chatbot Arena (Kaggle, 2024) ‚Äî 31/950 ü•à\n\nChallenge was to develop a reward model (used in RLHF stage) for multilingual human conversations on the chatbot arena (formerly LMSYS).\nFinetuned LLMs as reward models in classification setting and used various techniques like multi-stage training (pretraining, finetuning), pseudo labelling, LoRA, QLoRA, efficient inference techniques, knowledge distillation.\nCompetition Link | Code\n\n\n\nKaggle ‚Äî LLM Science Exam (2023) ‚Äî 123/2664 ü•à\n\nMultiple-choice science questions; evaluated by MAP@3.\nDual retrieval: TF-IDF over curated Wikipedia corpora + dense retrieval (BGE-small-en v1.5) with FAISS.\nContext assembly: join top-K passages into a single context string per question.\nAnswering model: DeBERTa-v3-Large as multiple-choice scorer (context + question paired with each option).\nEnsembling: soft-average logits across checkpoints √ó retrieval variants for the final MAP@3 submission.\nCompetition Link\n\n\n\nDataSolve-India (Wolters Kluwer, 2022) ‚Äî 1st place ü•á\n\nThe objective was to categorize regulations that are crucial for business compliance (multi-label classification).\nUsed weighted hill-climbing ensemble of transformer models (DeBERTa-v3, RoBERTa) and GBDT‚Äôs (CatBoost, XGBoost).\nCompetition Link | Code\n\n\n\nU.S. Patent Phrase-to-Phrase Matching (Kaggle, 2022) ‚Äî 31/1889 ü•à\n\nThe task was to extract relevant information by determining the semantic similarity between key phrases in patent documents.\nUsed hill-climbing ensemble technique, and a range of transformer models trained using different strategies to ensure diversity.\nCompetition Link | Code\n\n\n\nHappywhale ‚Äî Whale & Dolphin Identification (Kaggle, 2022) ‚Äî 132/1588 ü•â\n\nIndividual re-identification using dorsal fin/marking signatures.\nCurated and published resized training sets to accelerate iteration/stabilize training.\nBuilt a robust visual re-ID pipeline (embedding model + nearest-neighbour matching), with heavy augmentation and careful per-individual CV to avoid leakage.\nIterated on mining schedules and validation sanity checks for consistent generalization.\nCompetition Link\n\n\n\nAmazon ML Challenge (HackerEarth, 2021) ‚Äî 11/3294\n\nThe task required categorizing products into browse node IDs for a large dataset consisting of 2.67GB of text and with 9k+ classes.\nUsed standard handcrafted features, sentence embeddings, TF-IDF and a custom neural network to merge all features.\nCompetition Link | Code (team)\n\n\n\nBristol-Myers Squibb ‚Äî Molecular Translation (Kaggle, 2021) ‚Äî 50/874 ü•à\n\nThe task was to interpret old chemical images and convert images back to the underlying chemical structure annotated as InChI text.\nUsed Vision Transformer (ViT) as encoder and original transformer decoder.\nGenerated 12M synthetic images with RDKit for better ViT performance.\nCompetition Link\n\n\n\nSartorius ‚Äî Cell Instance Segmentation (Kaggle, 2021) ‚Äî 117/1505 ü•â\n\nDetect and delineate single neuronal cells in microscopy images (instance segmentation).\nImplemented Detectron2-based Mask R-CNN pipeline (with tuned anchors/thresholds) as the primary model.\nAdded a parallel Cellpose track for comparison; tracked results and failures to guide post-processing.\nTTA (flips/scales) and morphological post-processing (small-object cleanup, hole-filling) to refine masks.\nDeployed a small demo app for qualitative review and error analysis.\nCompetition Link | Code"
  },
  {
    "objectID": "pages/about.html#open-source-contributions",
    "href": "pages/about.html#open-source-contributions",
    "title": "About",
    "section": "üåç Open-Source Contributions",
    "text": "üåç Open-Source Contributions\n\nFixed an example script on HuggingFace ü§ó transformers repository for XLA devices - PR\nMade the experiment trackers to launch only on main process in distributed setups on ü§ó Accelerate library - PR\nFixed several examples and removed the check for main process‚Ä¶ on ü§ó Accelerate library - PR\nUpdate several ü§ó transformers no_trainer scripts leveraging ü§ó Accelerate‚Ä¶ - PR\nContributed a report to Weights & Biases showcasing the integration of MONAI and W&B - PR\n\n\n\nüé§ Talks\nI frequently give talks on machine learning and MLOps topics. Most of my talks to colleges and small groups are not recorded, but here is one notable recorded presentation:\n\nWeights & Biases MLOps Conference (Fully Connected 2023) - I was a speaker at the inaugural Weights & Biases MLOps conference. You can listen to my talk here. Also here‚Äôs the announcement post.\n\nI‚Äôm open to giving talks! If you‚Äôre interested in having me speak at your event, please reach out to me at atharvaaingle@gmail.com."
  },
  {
    "objectID": "posts/another-test.html",
    "href": "posts/another-test.html",
    "title": "My Second Blog Post",
    "section": "",
    "text": "I was just watching a Sentdex video and found out this pretty cool example on where we could use Pipeline Parallelism on his Jetson Thor which has a slow bandwith typically compared to a production GPU. This is a really good example of it. Here‚Äôs what was the moat of that approach:\n\nBottleneck: The Jetson Thor‚Äôs low memory bandwidth (273 GB/s) caused per-request latency (~500 ms) in the MoonDream 2 VLM, limiting throughput to about 2 FPS. It wasn‚Äôt compute or networking; ‚Äúwe know the bottleneck is the memory bandwidth.‚Äù\nFix (pipeline-style parallelism): He exploited the large 128 GB memory by running many independent VLM servers in parallel (each ~5 GB). Frames were interleaved across servers (1,3,5‚Ä¶ to server A; 2,4,6‚Ä¶ to server B; etc.). This keeps single-frame latency the same but increases overall frame throughput.\nResult: Doubling servers doubled FPS; 10 servers ‚âà 20 FPS; 15 servers ‚âà 30 FPS, with high GPU utilization (often 90%+). Trade-off: fewer headroom for other models, but you can tune server count."
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blog",
    "section": "",
    "text": "All blogs written by me (includes external blogs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy First Blog Post\n\n\n\nnew\n\n\n\nA brief description of what this post is about\n\n\n\n\n\nSep 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMy Second Blog Post\n\n\n\nLLMs\n\n\n\nA brief description of what this post is about\n\n\n\n\n\nSep 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMUVERA - Google‚Äôs FDE approach for speeding up multi-vector retrieval\n\n\n\nW&B\n\nRAG\n\n\n\nScale search with MUVERA: Google‚Äôs FDE approach cuts latency 18x while W&B observability ensures reliable, trackable experiments.\n\n\n\n\n\nAug 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Google‚Äôs Agent Development Kit and Agent2Agent\n\n\n\nW&B\n\nAgents\n\n\n\nDiscover the Agent2Agent (A2A) open protocol for AI interoperability. Learn A2A principles & build a multi-agent system using Google‚Äôs ADK in our tutorial.\n\n\n\n\n\nApr 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDeep reinforcement learning: Integrating neural networks with RL\n\n\n\nW&B\n\nRL\n\n\n\nExplore how deep reinforcement learning combines neural networks and RL to enable agents to learn optimal strategies from raw data across gaming and robotics.\n\n\n\n\n\nApr 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSupervised learning vs deep learning vs reinforcement learning\n\n\n\nW&B\n\nRL\n\n\n\nSupervised vs deep vs reinforcement learning explained. See how AI uses labels, networks & rewards to learn, plus deep reinforcement learning examples.\n\n\n\n\n\nApr 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs are machine learning classifiers\n\n\n\nW&B\n\nLLMs\n\n\n\nLearn how to use LLMs like GPT for text classification. Explore prompting, fine-tuning, and when to choose LLMs over traditional machine learning classifiers.\n\n\n\n\n\nJan 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRAG techniques: From naive to advanced\n\n\n\nW&B\n\nRAG\n\n\n\nExplore various RAG techniques, from basic to advanced, and discover how chunking, indexing, and query transformation can elevate your AI‚Äôs performance in complex use cases.\n\n\n\n\n\nOct 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAI and regulatory compliance in finance\n\n\n\nW&B\n\nFinance\n\n\n\nExplore regulatory compliance in finance, types of regulations, risks of non-compliance through real-world examples, and the role of technology and AI in safeguarding the financial world.\n\n\n\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is model risk management in finance?\n\n\n\nW&B\n\nFinance\n\n\n\nThis guide covers why Model Risk Management (MRM) is important to ensure financial models are accurate and safe, and provides tips for managing model risks effectively.\n\n\n\n\n\nApr 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Q&A Bot for Weights & Biases\n\n\n\nW&B\n\nRAG\n\n\n\nIn this article, we explore how to utilize OpenAI‚Äôs ChatGPT and LangChain to build a Question-Answering bot for Weights & Biases‚Äô podcast series, Gradient Dissent.\n\n\n\n\n\nApr 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBLIP-2: A new Visual Language Model by Salesforce\n\n\n\nW&B\n\nVLMs\n\n\n\nIn this article, we‚Äôll explore BLIP-2, a new Vision Language Model by Salesforce which beats the previous state-of-the-art Flamingo on various multimodal tasks.\n\n\n\n\n\nFeb 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIs PyTorch 2.0 Faster Than PyTorch 1.13?\n\n\n\nW&B\n\nPytorch\n\n\n\nIn this article, we make a comparison between the current stable version of PyTorch 1.13 with the latest announced PyTorch 2.0 to see which performs better.\n\n\n\n\n\nJan 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSetFit: Efficient Few-Shot Learning Without Prompts\n\n\n\nW&B\n\n\n\nPublish your model insights with interactive plots for performance metrics, predictions, and hyperparameters. Made by Atharva Ingle using Weights & Biases\n\n\n\n\n\nNov 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDeepMind Flamingo: A Visual Language Model for Few-Shot Learning\n\n\n\nW&B\n\nVLMs\n\n\n\nIn this article, we‚Äôll explore Flamingo ‚Äî an open-ended single visual language model (VLM) for multimodal machine learning research developed by DeepMind.\n\n\n\n\n\nNov 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHugging Face Accelerate Super Charged With Weights & Biases\n\n\n\nW&B\n\nPytorch\n\n\n\nIn this article, we‚Äôll walk through how to use Hugging Face Accelerate with W&B, demonstrating how easy it is to perform distributed training and evaluation.\n\n\n\n\n\nOct 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n3D Segmentation with MONAI and PyTorch Supercharged by Weights & Biases\n\n\n\nW&B\n\nPytorch\n\n\n\nA tutorial on how to use Weights & Biases with MONAI and PyTorch to accelerate your medical research. Made by Atharva Ingle using Weights & Biases\n\n\n\n\n\nAug 16, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/testing-my-own-blog.html",
    "href": "posts/testing-my-own-blog.html",
    "title": "My First Blog Post",
    "section": "",
    "text": "vLLM V0 + Prefix Caching on 2√óT4: Why merged weights hung but LoRA didn‚Äôt\n\n\nTL;DR\n\nYour freeze was a V0 engine + TP&gt;1 + prefix caching interaction under tight VRAM. Turning enable_prefix_caching=False removed that path.\nWith merged weights, the engine reused one giant shared-prefix KV cache across thousands of requests; the V0 scheduler/block-manager then ran out of allocatable KV blocks and stalled. With LoRA, caches are namespaced per adapter, so that aggressive sharing path wasn‚Äôt taken, avoiding the stall.\n\n\n\n\nWhat prefix caching is doing in vLLM\n\nPrefix caching stores KV-cache blocks for a shared prompt prefix so later requests can skip prefill for those tokens; it‚Äôs implemented via block hashing + LRU over KV blocks.\nIn V0/V1 design notes and RFCs, cached KV blocks are treated like an OS cache: blocks are retained instead of freed and tracked in a global structure for reuse.\n\n\n\n\nWhy merged weights + prefix caching froze on your run\n\nScheduler blind spot (V0 path):\nIn V0, the scheduler decides whether it can allocate all KV blocks for a sequence before it accounts for which blocks are already cached. So even when most of your long prefix is cacheable, the scheduler may still demand a large number of free blocks up front. With thousands of identical prompts, this starves the block manager and the run stalls.\nTight VRAM ‚Üí no headroom:\nvLLM pre-allocates KV blocks up to a fraction of GPU memory (gpu_memory_utilization). With 16 GiB T4s, TP&gt;1, long context, and high utilization, the pool has little slack; coupled with (1), you hit a deadlock/starvation pattern (first request OK, then nothing). Lowering utilization or disabling prefix caching frees the pipeline.\nThis exact failure mode is known:\nMultiple reports show hangs/crashes when prefix caching is enabled, where ‚Äúfirst request is fine, second returns nothing‚Äù. Disabling prefix caching or reducing memory pressure is the workaround.\n\n\n\n\nWhy LoRA didn‚Äôt show the problem (but merged weights did)\n\nWith multiple adapters, vLLM treats KV caches as separate per LoRA (cache keys include the adapter identity), so sequences using an adapter don‚Äôt reuse the base-model prefix cache. That reduces cache sharing pressure and avoids the problematic allocator/scheduler path on V0. After you merged LoRA ‚Üí base, there‚Äôs one weight set again, so aggressive prefix reuse kicked in and exposed the stall.\n\n\n\n\nWhat to do on Kaggle 2√óT4 (V0 engine)\n\nKeep enable_prefix_caching=False when you have a long, identical prompt and TP&gt;1.\nLeave VRAM headroom: gpu_memory_utilization‚âà0.80, trim max_model_len to what you actually need, and batch in small chunks (32‚Äì128 prompts per generate). These align with vLLM‚Äôs guidance to avoid KV-block starvation.\n\n\n\n\nNotes on V1 vs V0 (why you read conflicting advice)\n\nV1 reworked prefix caching data structures; it‚Äôs near-zero overhead and on by default in many builds, but you‚Äôre on T4 ‚Üí V0 fallback, so you still hit the older scheduler/block-manager behavior. Some V1 docs even suggest toggling prefix caching/eager mode depending on model/backend. The takeaway: your hardware keeps you on V0, so use the V0-safe settings above.\n\n\n\n\nOne-line mental model\nMerged weights made all requests share one hot prefix cache ‚Üí V0 scheduler demanded more free KV blocks than available ‚Üí starvation. LoRA segmented caches by adapter ‚Üí less reuse ‚Üí no starvation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Atharva Ingle",
    "section": "",
    "text": "I‚Äôm Atharva, a Machine Learning Engineer focused on LLMs, RAG and AI agents. I like building things that ship, writing what I learn, and competing on Kaggle for fun. This site collects my notes, projects, and occasional essays. See my about page to read about my work in much detail."
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Atharva Ingle",
    "section": "üìÆ Blog",
    "text": "üìÆ Blog\nRecent posts and notes:\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\n\n\n\n\n\n9/7/25\n\n\nMy First Blog Post\n\n\n\n\n\n\n9/7/25\n\n\nMy Second Blog Post\n\n\n\n\n\n\n8/8/25\n\n\nMUVERA - Google‚Äôs FDE approach for speeding up multi-vector retrieval\n\n\n\n\n\n\n4/10/25\n\n\nUsing Google‚Äôs Agent Development Kit and Agent2Agent\n\n\n\n\n\n\n4/7/25\n\n\nDeep reinforcement learning: Integrating neural networks with RL\n\n\n\n\n\n\n4/5/25\n\n\nSupervised learning vs deep learning vs reinforcement learning\n\n\n\n\n\n\n1/27/25\n\n\nLLMs are machine learning classifiers\n\n\n\n\n\n\n10/5/24\n\n\nRAG techniques: From naive to advanced\n\n\n\n\n\n\n5/1/24\n\n\nAI and regulatory compliance in finance\n\n\n\n\n\n\n4/29/24\n\n\nWhat is model risk management in finance?\n\n\n\n\n\n\n4/25/23\n\n\nBuilding a Q&A Bot for Weights & Biases\n\n\n\n\n\n\n2/24/23\n\n\nBLIP-2: A new Visual Language Model by Salesforce\n\n\n\n\n\n\n1/24/23\n\n\nIs PyTorch 2.0 Faster Than PyTorch 1.13?\n\n\n\n\n\n\n11/29/22\n\n\nSetFit: Efficient Few-Shot Learning Without Prompts\n\n\n\n\n\n\n11/16/22\n\n\nDeepMind Flamingo: A Visual Language Model for Few-Shot Learning\n\n\n\n\n\n\n10/15/22\n\n\nHugging Face Accelerate Super Charged With Weights & Biases\n\n\n\n\n\n\n8/16/22\n\n\n3D Segmentation with MONAI and PyTorch Supercharged by Weights & Biases\n\n\n\n\n\n\nNo matching items"
  }
]