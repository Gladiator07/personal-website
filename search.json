[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Atharva Ingle",
    "section": "",
    "text": "I’m Atharva, a Machine Learning Engineer focused on LLMs, RAG and AI agents. I like building things that ship, writing what I learn, and competing on Kaggle for fun. This site collects my notes, projects, and occasional essays. See my about page to read about my past work in much detail. Also check out my blog where I write about what I learn occasionally."
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Atharva Ingle",
    "section": "📮 Blog",
    "text": "📮 Blog\nRecent posts and notes. You can subscribe to my RSS Feed here:\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\n\n\n\n\n\n8/8/25\n\n\nMUVERA - Google’s FDE approach for speeding up multi-vector retrieval\n\n\n\n\n\n\n4/10/25\n\n\nUsing Google’s Agent Development Kit and Agent2Agent\n\n\n\n\n\n\n4/7/25\n\n\nDeep reinforcement learning: Integrating neural networks with RL\n\n\n\n\n\n\n4/5/25\n\n\nSupervised learning vs deep learning vs reinforcement learning\n\n\n\n\n\n\n1/27/25\n\n\nLLMs are machine learning classifiers\n\n\n\n\n\n\n10/5/24\n\n\nRAG techniques: From naive to advanced\n\n\n\n\n\n\n5/1/24\n\n\nAI and regulatory compliance in finance\n\n\n\n\n\n\n4/29/24\n\n\nWhat is model risk management in finance?\n\n\n\n\n\n\n4/25/23\n\n\nBuilding a Q&A Bot for Weights & Biases\n\n\n\n\n\n\n2/24/23\n\n\nBLIP-2: A new Visual Language Model by Salesforce\n\n\n\n\n\n\n1/24/23\n\n\nIs PyTorch 2.0 Faster Than PyTorch 1.13?\n\n\n\n\n\n\n11/29/22\n\n\nSetFit: Efficient Few-Shot Learning Without Prompts\n\n\n\n\n\n\n11/16/22\n\n\nDeepMind Flamingo: A Visual Language Model for Few-Shot Learning\n\n\n\n\n\n\n10/15/22\n\n\nHugging Face Accelerate Super Charged With Weights & Biases\n\n\n\n\n\n\n8/16/22\n\n\n3D Segmentation with MONAI and PyTorch Supercharged by Weights & Biases\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blog",
    "section": "",
    "text": "All blogs written by me (includes external blogs). You can subscribe to my RSS Feed here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMUVERA - Google’s FDE approach for speeding up multi-vector retrieval\n\n\n\nW&B\n\nRAG\n\n\n\nScale search with MUVERA: Google’s FDE approach cuts latency 18x while W&B observability ensures reliable, trackable experiments.\n\n\n\n\n\nAug 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Google’s Agent Development Kit and Agent2Agent\n\n\n\nW&B\n\nAgents\n\n\n\nDiscover the Agent2Agent (A2A) open protocol for AI interoperability. Learn A2A principles & build a multi-agent system using Google’s ADK in our tutorial.\n\n\n\n\n\nApr 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDeep reinforcement learning: Integrating neural networks with RL\n\n\n\nW&B\n\nRL\n\n\n\nExplore how deep reinforcement learning combines neural networks and RL to enable agents to learn optimal strategies from raw data across gaming and robotics.\n\n\n\n\n\nApr 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSupervised learning vs deep learning vs reinforcement learning\n\n\n\nW&B\n\nRL\n\n\n\nSupervised vs deep vs reinforcement learning explained. See how AI uses labels, networks & rewards to learn, plus deep reinforcement learning examples.\n\n\n\n\n\nApr 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs are machine learning classifiers\n\n\n\nW&B\n\nLLMs\n\n\n\nLearn how to use LLMs like GPT for text classification. Explore prompting, fine-tuning, and when to choose LLMs over traditional machine learning classifiers.\n\n\n\n\n\nJan 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRAG techniques: From naive to advanced\n\n\n\nW&B\n\nRAG\n\n\n\nExplore various RAG techniques, from basic to advanced, and discover how chunking, indexing, and query transformation can elevate your AI’s performance in complex use cases.\n\n\n\n\n\nOct 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAI and regulatory compliance in finance\n\n\n\nW&B\n\nFinance\n\n\n\nExplore regulatory compliance in finance, types of regulations, risks of non-compliance through real-world examples, and the role of technology and AI in safeguarding the financial world.\n\n\n\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is model risk management in finance?\n\n\n\nW&B\n\nFinance\n\n\n\nThis guide covers why Model Risk Management (MRM) is important to ensure financial models are accurate and safe, and provides tips for managing model risks effectively.\n\n\n\n\n\nApr 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Q&A Bot for Weights & Biases\n\n\n\nW&B\n\nRAG\n\n\n\nIn this article, we explore how to utilize OpenAI’s ChatGPT and LangChain to build a Question-Answering bot for Weights & Biases’ podcast series, Gradient Dissent.\n\n\n\n\n\nApr 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBLIP-2: A new Visual Language Model by Salesforce\n\n\n\nW&B\n\nVLMs\n\n\n\nIn this article, we’ll explore BLIP-2, a new Vision Language Model by Salesforce which beats the previous state-of-the-art Flamingo on various multimodal tasks.\n\n\n\n\n\nFeb 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIs PyTorch 2.0 Faster Than PyTorch 1.13?\n\n\n\nW&B\n\nPytorch\n\n\n\nIn this article, we make a comparison between the current stable version of PyTorch 1.13 with the latest announced PyTorch 2.0 to see which performs better.\n\n\n\n\n\nJan 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSetFit: Efficient Few-Shot Learning Without Prompts\n\n\n\nW&B\n\n\n\nPublish your model insights with interactive plots for performance metrics, predictions, and hyperparameters. Made by Atharva Ingle using Weights & Biases\n\n\n\n\n\nNov 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDeepMind Flamingo: A Visual Language Model for Few-Shot Learning\n\n\n\nW&B\n\nVLMs\n\n\n\nIn this article, we’ll explore Flamingo — an open-ended single visual language model (VLM) for multimodal machine learning research developed by DeepMind.\n\n\n\n\n\nNov 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHugging Face Accelerate Super Charged With Weights & Biases\n\n\n\nW&B\n\nPytorch\n\n\n\nIn this article, we’ll walk through how to use Hugging Face Accelerate with W&B, demonstrating how easy it is to perform distributed training and evaluation.\n\n\n\n\n\nOct 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n3D Segmentation with MONAI and PyTorch Supercharged by Weights & Biases\n\n\n\nW&B\n\nPytorch\n\n\n\nA tutorial on how to use Weights & Biases with MONAI and PyTorch to accelerate your medical research. Made by Atharva Ingle using Weights & Biases\n\n\n\n\n\nAug 16, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This page contains a detailed overview of the past work I did."
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "About",
    "section": "💼 Work Experience",
    "text": "💼 Work Experience\n\nWolters Kluwer (Jan 2023 - Present) - Machine Learning Engineer\n\nBorrower Analytics 2.0 (UCC filings)\n\nProcessed 65M UCC filings for information extraction from PDFs (&gt;1B single-page images).\nFine-tuned and deployed a vision-language model to extract 50+ fields from UCC1/UCC3 across diverse state formats; achieved 95% extraction accuracy.\nDeployed with a high-throughput serving stack at ~10 images/sec on H100 GPUs.\nBuilt an LLM-as-a-judge framework to generate high-quality labeled data, significantly reducing human labeling effort.\nTrained and deployed a transformer for text segmentation (97% recall) extracting complete collateral text.\nTrained a lien-classification model (95% accuracy) based on collateral text.\n\nIRA Knowledge-Base Chatbot (RAG)\n\nBuilt a RAG chatbot over an XML-based IRA knowledge base for faster query resolution.\nImplemented query transformation, parent–child retrieval, hierarchical indexing, and multi-vector retrieval in LanceDB.\nImproved retrieval with ColBERT reranking and hybrid search (semantic + BM25).\nReached ~97% CSAT, improving user experience and response accuracy.\n\nWK AI Studio (internal fine-tuning platform)\n\nBuilt an end-to-end internal framework to fine-tune and deploy models to production.\nEnabled non-technical teams to fine-tune models with their own data.\nIntegrated MLflow with full lineage, model registry, and dataset/model versioning for production tracking.\nSupported distributed multi-GPU, 4-/8-bit training, LoRA/QLoRA, mixed precision, and differential learning rates, plus other SOTA techniques.\n\nBLMS (Business License Match/Search)\n\nBuilt a search engine to recommend required licenses for starting a business.\nDeveloped a HyDE (Hypothetical Document Embeddings) workflow to expand short user queries into meaningful search text.\nUsed synthetically generated descriptions from an internal taxonomy and a reranker, achieving 96% retrieval accuracy.\n\nProviso (legal citations chatbot)\n\nBuilt a RAG-based chatbot on 91k legal citations for efficient legal query resolution.\nEnhanced retrieval with metadata filtering, query transformation, and similar techniques as IRA.\nUsed BERTTopic to generate hypothetical topics, which were leveraged for metadata filtering to reduce search space and improve query handling.\n\nEarlier — Data Science Intern (Jan–Jul 2023)\n\nBuilt an in-house key information extraction solution using a transformer-based stack.\nShipped a document classification pipeline.\n\n\n\n\nWeights & Biases (May 2022 - Present) - Ambassador\n\nEngineered optimized Kaggle notebooks with integrated W&B tracking and monitoring.\nAuthored technical reports showcasing W&B across medical imaging, visual-language models (Flamingo, BLIP-2), few-shot learning (SetFit), PyTorch 2.0, Hugging Face, RAG, and LLMs.\nYou can find all my W&B blogs here"
  },
  {
    "objectID": "about.html#competitions",
    "href": "about.html#competitions",
    "title": "About",
    "section": "📈 Competitions",
    "text": "📈 Competitions\nI love participating in machine learning competitions. I am primarily active on Kaggle and am a Kaggle Competition Expert.\n\nWSDM Cup — Multilingual Chatbot Arena (Kaggle, 2024) — 31/950 🥈\n\nChallenge was to develop a reward model (used in RLHF stage) for multilingual human conversations on the chatbot arena (formerly LMSYS).\nFinetuned LLMs as reward models in classification setting and used various techniques like multi-stage training (pretraining, finetuning), pseudo labelling, LoRA, QLoRA, efficient inference techniques, knowledge distillation.\nCompetition Link | Code\n\n\n\nKaggle — LLM Science Exam (2023) — 123/2664 🥈\n\nMultiple-choice science questions; evaluated by MAP@3.\nDual retrieval: TF-IDF over curated Wikipedia corpora + dense retrieval (BGE-small-en v1.5) with FAISS.\nContext assembly: join top-K passages into a single context string per question.\nAnswering model: DeBERTa-v3-Large as multiple-choice scorer (context + question paired with each option).\nEnsembling: soft-average logits across checkpoints × retrieval variants for the final MAP@3 submission.\nCompetition Link\n\n\n\nDataSolve-India (Wolters Kluwer, 2022) — 1st place 🥇\n\nThe objective was to categorize regulations that are crucial for business compliance (multi-label classification).\nUsed weighted hill-climbing ensemble of transformer models (DeBERTa-v3, RoBERTa) and GBDT’s (CatBoost, XGBoost).\nCompetition Link | Code\n\n\n\nU.S. Patent Phrase-to-Phrase Matching (Kaggle, 2022) — 31/1889 🥈\n\nThe task was to extract relevant information by determining the semantic similarity between key phrases in patent documents.\nUsed hill-climbing ensemble technique, and a range of transformer models trained using different strategies to ensure diversity.\nCompetition Link | Code\n\n\n\nHappywhale — Whale & Dolphin Identification (Kaggle, 2022) — 132/1588 🥉\n\nIndividual re-identification using dorsal fin/marking signatures.\nCurated and published resized training sets to accelerate iteration/stabilize training.\nBuilt a robust visual re-ID pipeline (embedding model + nearest-neighbour matching), with heavy augmentation and careful per-individual CV to avoid leakage.\nIterated on mining schedules and validation sanity checks for consistent generalization.\nCompetition Link\n\n\n\nAmazon ML Challenge (HackerEarth, 2021) — 11/3294\n\nThe task required categorizing products into browse node IDs for a large dataset consisting of 2.67GB of text and with 9k+ classes.\nUsed standard handcrafted features, sentence embeddings, TF-IDF and a custom neural network to merge all features.\nCompetition Link | Code (team)\n\n\n\nBristol-Myers Squibb — Molecular Translation (Kaggle, 2021) — 50/874 🥈\n\nThe task was to interpret old chemical images and convert images back to the underlying chemical structure annotated as InChI text.\nUsed Vision Transformer (ViT) as encoder and original transformer decoder.\nGenerated 12M synthetic images with RDKit for better ViT performance.\nCompetition Link\n\n\n\nSartorius — Cell Instance Segmentation (Kaggle, 2021) — 117/1505 🥉\n\nDetect and delineate single neuronal cells in microscopy images (instance segmentation).\nImplemented Detectron2-based Mask R-CNN pipeline (with tuned anchors/thresholds) as the primary model.\nAdded a parallel Cellpose track for comparison; tracked results and failures to guide post-processing.\nTTA (flips/scales) and morphological post-processing (small-object cleanup, hole-filling) to refine masks.\nDeployed a small demo app for qualitative review and error analysis.\nCompetition Link | Code"
  },
  {
    "objectID": "about.html#open-source-contributions",
    "href": "about.html#open-source-contributions",
    "title": "About",
    "section": "🌍 Open-Source Contributions",
    "text": "🌍 Open-Source Contributions\n\nFixed an example script on HuggingFace 🤗 transformers repository for XLA devices - PR\nMade the experiment trackers to launch only on main process in distributed setups on 🤗 Accelerate library - PR\nFixed several examples and removed the check for main process… on 🤗 Accelerate library - PR\nUpdate several 🤗 transformers no_trainer scripts leveraging 🤗 Accelerate… - PR\nContributed a report to Weights & Biases showcasing the integration of MONAI and W&B - PR"
  },
  {
    "objectID": "about.html#talks",
    "href": "about.html#talks",
    "title": "About",
    "section": "🎤 Talks",
    "text": "🎤 Talks\nI frequently give talks on machine learning and MLOps topics. Most of my talks to colleges and small groups are not recorded, but here is one notable recorded presentation:\n\nWeights & Biases MLOps Conference (Fully Connected 2023) - I was a speaker at the inaugural Weights & Biases MLOps conference. You can listen to my talk here. Also here’s the announcement post.\n\nI’m open to giving talks! If you’re interested in having me speak at your event, please reach out to me at atharvaaingle@gmail.com."
  }
]